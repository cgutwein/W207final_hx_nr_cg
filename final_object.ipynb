{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import math\n",
    "from IPython.display import display, clear_output \n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "\n",
    "# set random seed to achieve same results each time\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialKeypoints():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_csv = pd.read_csv(\"training.csv\")\n",
    "        self.test_csv = pd.read_csv(\"test.csv\")\n",
    "        self.sample_csv = pd.read_csv(\"SampleSubmission.csv\")\n",
    "        self.ID_csv = pd.read_csv(\"IdLookupTable.csv\")\n",
    "        self.label_names = self.train_csv.columns.tolist()[:-1]\n",
    "        self.dev_data_n, self.dev_labels_n, self.train_data_n, self.train_labels_n = self.create_training(self.train_csv)\n",
    "        self.dev_data, self.dev_labels, self.train_data, self.train_labels = self.create_training(self.train_csv, nans=False)\n",
    "        self.knn = self.K_nn(self.train_data, self.train_labels)\n",
    "        self.mlp = self.MLPRegressor(self.train_data, self.train_labels)\n",
    "    \n",
    "    def create_training(self, train_csv, dev_size=1000, nans=True):\n",
    "        if nans == False:\n",
    "            train_csv = train_csv.dropna()\n",
    "        labels = train_csv.loc[:, train_csv.columns != 'Image'].values\n",
    "        data = train_csv['Image'].str.split()\n",
    "        data = np.vstack(data.apply(lambda row: np.asarray([int(n) for n in row])).values)\n",
    "        data = data/255.0                                           # Rescale grayscale values to [0,1].\n",
    "        shuffle = np.random.permutation(np.arange(data.shape[0]))   # Shuffle the data\n",
    "        data, labels              = data[shuffle], labels[shuffle]  # Splitting into dev and training\n",
    "        dev_data, dev_labels      = data[:dev_size], labels[:dev_size]\n",
    "        train_data, train_labels  = data[dev_size:], labels[dev_size:]\n",
    "        return dev_data, dev_labels, train_data, train_labels\n",
    "    \n",
    "    def plot_example(self, data, label, predicted_label=np.nan):\n",
    "        plt.imshow(data.reshape(96, 96), cmap='gray')\n",
    "        plt.scatter(label[0::2], label[1::2], c='red', marker='x', label='actual')\n",
    "        if np.all(np.isfinite(predicted_label)):\n",
    "            plt.scatter(predicted_label[0::2], predicted_label[1::2], c='blue', marker='x', label='predicted')\n",
    "        plt.axis('off')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=12)\n",
    "        return\n",
    "    \n",
    "    def score(self, y_pred, y_actual):\n",
    "        return np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "    \n",
    "    def preprocessor():\n",
    "        # label_nan_filler() generates a substitute value for each missing facial keypoint as the average distance from the\n",
    "        # facial keypoint to the origin, scaling each distance by each image keypoints' Euclidean norm prior to\n",
    "        # averaging, then descaling for the final value    \n",
    "        def label_nan_filler():\n",
    "            # the line below returns a list of the number of missing label values for each facial keypoint\n",
    "            train_label_nan_counts = np.count_nonzero(np.isnan(self.train_labels_n), axis = 0)\n",
    "    \n",
    "            # get the indices of keypoints that have no missing label values, these will be the origin\n",
    "            origin_index = [i for i, x in enumerate(train_label_nan_counts) if x == 0]\n",
    "\n",
    "            # extract the values for each image using the origin index\n",
    "            origin_x = self.train_labels_n[:, origin_index[0]]\n",
    "            origin_y = self.train_labels_n[:, origin_index[1]]\n",
    "\n",
    "            # separate x labels from y labels\n",
    "            x_keypoints = self.train_labels_n[:,0::2]\n",
    "            y_keypoints = self.train_labels_n[:,1::2]\n",
    "\n",
    "            # get the distance between each point and the origin\n",
    "            x_keypoint_dist_from_origin = np.empty((x_keypoints.shape))\n",
    "            y_keypoint_dist_from_origin = np.empty((y_keypoints.shape))\n",
    "            for i in range(x_keypoints.shape[0]):\n",
    "                for j in range(x_keypoints.shape[1]):\n",
    "                    x_keypoint_dist_from_origin[i,j] = x_keypoints[i,j] - origin_x[i]\n",
    "                    y_keypoint_dist_from_origin[i,j] = y_keypoints[i,j] - origin_y[i]\n",
    "\n",
    "            # get the Euclidean norm of each distance from origin (with as many non-nan values as possible) to use as\n",
    "            # a scaling factor\n",
    "            label_norms = np.empty(len(self.train_labels_n))\n",
    "            for i in range(len(self.train_labels_n)):\n",
    "                label_norms[i] = np.linalg.norm(np.hstack((x_keypoint_dist_from_origin[i, ~np.isnan(x_keypoint_dist_from_origin[i])],\n",
    "                                                           y_keypoint_dist_from_origin[i, ~np.isnan(y_keypoint_dist_from_origin[i])])))\n",
    "\n",
    "            # scale each facial keypoint distance from origin by the Eucliean norm of its associated face\n",
    "            # initiate distance from origins for x and y\n",
    "            x_scaled_dist_from_origin_list = np.empty((x_keypoints.shape))\n",
    "            y_scaled_dist_from_origin_list = np.empty((y_keypoints.shape))\n",
    "            for i in range(len(self.train_labels_n)):\n",
    "                for j in range(int(len(self.label_names)/2)):\n",
    "                    x_scaled_dist_from_origin_list[i,j] = x_keypoint_dist_from_origin[i,j]/label_norms[i]\n",
    "                    y_scaled_dist_from_origin_list[i,j] = y_keypoint_dist_from_origin[i,j]/label_norms[i]\n",
    "            \n",
    "            # convert list of arrays into one array (for x and y)\n",
    "            x_scaled_dist_from_origin = np.array(x_scaled_dist_from_origin_list)\n",
    "            y_scaled_dist_from_origin = np.array(y_scaled_dist_from_origin_list)\n",
    "            \n",
    "            # take the mean of each scaled facial keypoint across all faces\n",
    "            x_mean_scaled_dist_from_origin = np.nanmean(x_scaled_dist_from_origin, axis=0)\n",
    "            y_mean_scaled_dist_from_origin = np.nanmean(y_scaled_dist_from_origin, axis=0)\n",
    "            \n",
    "            # combine the x and y mean scaled distances from the origin into the original label order\n",
    "            mean_scaled_dist_from_origin = np.empty(30)\n",
    "            x_iter = iter(x_mean_scaled_dist_from_origin)\n",
    "            y_iter = iter(y_mean_scaled_dist_from_origin)\n",
    "            for i in range(len(mean_scaled_dist_from_origin)):\n",
    "                if i%2 == 0:\n",
    "                    mean_scaled_dist_from_origin[i] = next(x_iter)\n",
    "                else:\n",
    "                    mean_scaled_dist_from_origin[i] = next(y_iter)\n",
    "            \n",
    "            # substitute missing values for average values\n",
    "            filled_in_train_labels = np.empty((self.train_labels_n).shape)\n",
    "            # substitute value distance from origin scale factor (multiplied by norm of labels)\n",
    "            s_f = 2.2\n",
    "            # add a y offset, since missing value noses tend to be labeled at too low of a position\n",
    "            y_off = -5\n",
    "            # check for nan values\n",
    "            for row in range(len(self.train_labels_n)):\n",
    "                for label_index in range(len(self.label_names)):\n",
    "                    if math.isnan(self.train_labels_n[row, label_index]):\n",
    "                        # add de-scaled mean distance from x or y origin for average facial keypoint position to each NaN\n",
    "                        if label_index%2 == 0:\n",
    "                            filled_in_train_labels[row, label_index] = origin_x[row] + (mean_scaled_dist_from_origin[label_index] *s_f*label_norms[row])\n",
    "                        else:\n",
    "                            filled_in_train_labels[row, label_index] = origin_y[row] + y_off +(mean_scaled_dist_from_origin[label_index] *s_f*label_norms[row])\n",
    "                    # fill the rest of the array with the original label values\n",
    "                    else:\n",
    "                        filled_in_train_labels[row, label_index] = self.train_labels_n[row, label_index]\n",
    "            \n",
    "            # return array with no NaN values\n",
    "            return filled_in_train_labels\n",
    "        \n",
    "        preprocessed_data = label_nan_filler()\n",
    "        return preprocessed_data\n",
    "        return\n",
    "    \n",
    "    def generate_training():\n",
    "        return\n",
    "    \n",
    "    def blur_training():\n",
    "        return\n",
    "    \n",
    "    def K_nn(self, data, labels, n_neighbors=3):\n",
    "        knn = KNeighborsRegressor(n_neighbors)\n",
    "        knn.fit(data, labels)\n",
    "        return knn\n",
    "    \n",
    "    def LogReg(alpha):\n",
    "        return\n",
    "    \n",
    "    def MLPRegressor(self, data, labels):\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(300, 100))\n",
    "        mlp.fit(data, labels)\n",
    "        return mlp\n",
    "    \n",
    "    def _average_nans(self, train_labels):\n",
    "        df = pd.DataFrame(train_labels)\n",
    "        return df.fillna(df.mean()).values\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'training.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-06d94bdf61d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFacialKeypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-79aec9ddb971>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SampleSubmission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'training.csv' does not exist"
     ]
    }
   ],
   "source": [
    "self = FacialKeypoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that there are many examples that are missing a full label set. Out of the total 6049 examples, only 1140 have the full set of labels. Here is the percentage break out of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(self.train_csv.loc[:, self.train_csv.columns != 'Image'].count()/len(self.train_csv.index)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_labels = self.preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.plot_example(self.train_data_n[0], self.train_labels_n[0]) # from incomplete labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.plot_example(self.train_data_n[0], preprocessed_train_labels[0]) # from incomplete labels, filled in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.plot_example(self.train_data[25], self.train_labels[25]) # from complete labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = self.knn.predict(self.dev_data) # this is a stored method that trains on train data then returns KNN object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.score(self.dev_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using _average_nans to preprocess and create more training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.knn = self.K_nn(data=self.train_data_n, labels=self._average_nans(self.train_labels_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = self.knn.predict(self.dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.score(self.dev_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('RMSE of '+ str((self.score(self.dev_labels[25], y_pred[25]))))\n",
    "self.plot_example(self.dev_data[25], self.dev_labels[25], y_pred[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scale Training and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_scaler = StandardScaler()\n",
    "label_scaler.fit(self.train_labels)\n",
    "\n",
    "data_scaler = StandardScaler()\n",
    "data_scaler.fit(self.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.mlp = self.MLPRegressor(data_scaler.transform(self.train_data), label_scaler.transform(self.train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = self.mlp.predict(scaler.transform(self.dev_data))\n",
    "self.score(self.dev_labels, label_scaler.inverse_transform(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('RMSE of '+ str((self.score(self.dev_labels[25], label_scaler.inverse_transform(y_pred[25])))))\n",
    "self.plot_example(self.dev_data[25], self.dev_labels[25], label_scaler.inverse_transform(y_pred[25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search to Find Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "parameters ={'n_neighbors': range(1, 25)}\n",
    "clf = GridSearchCV(knn, parameters)\n",
    "clf.fit(self.train_data, self.train_labels)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
