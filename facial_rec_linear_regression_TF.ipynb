{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from IPython.display import display, clear_output \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training.csv which contains a header row of face locations and 'image,' the column of lists of pixel values\n",
    "with open(\"training.csv\") as f:\n",
    "    train_import = list(csv.reader(f))\n",
    "\n",
    "# shuffle the data    \n",
    "removed_header = train_import[1:]\n",
    "shuffle = np.random.permutation(np.arange(len(removed_header)))\n",
    "shuffled_train_import = [removed_header[i] for i in shuffle]\n",
    "    \n",
    "# divide the rows 80% for training and 20% for development    \n",
    "train_portion = int(.8*(len(shuffled_train_import)))\n",
    "dev_portion = len(shuffled_train_import) - train_portion\n",
    "\n",
    "# extract training and development data\n",
    "train_data = shuffled_train_import[:train_portion]\n",
    "dev_data = shuffled_train_import[-dev_portion:]\n",
    "\n",
    "# initialize the list of lists of pixel values \n",
    "train_image_data = []\n",
    "\n",
    "# convert each list of pixel values (1 per row) to integers (first split the string up then convert each elem to int)\n",
    "for row in range(len(train_data)):\n",
    "    train_image_data.append([int(i) for i in list(train_data[row][30].split())])\n",
    "# convert to a numpy array\n",
    "train_image_data = np.array(train_image_data)\n",
    "\n",
    "# convert range of 0 to 255 for pixel values to 0 to 1 (0 = white, 1 = black)\n",
    "train_image_data = train_image_data/255.0\n",
    "    \n",
    "# initialize array of the 30 different facial location target values (labels) for each example image\n",
    "train_image_labels = np.empty((len(train_data), 30))\n",
    "\n",
    "# iterate through each row and each example image's 30 target facial locations and covert to floats or NaNs\n",
    "for i in range(len(train_data)):\n",
    "    for j in range(len(train_data[i][0:30])):\n",
    "        if train_data[i][j] == '':\n",
    "            train_image_labels[i][j] = float('nan')\n",
    "        else: \n",
    "            train_image_labels[i][j] = (float(train_data[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with NaN Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where 'left_eye_center_x' == NaN values from the training and label data\n",
    "non_NaN_list = np.invert((np.isnan(train_image_labels[:,0])))\n",
    "cln_train_image_data = train_image_data[non_NaN_list]\n",
    "cln_train_image_labels = train_image_labels[non_NaN_list]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n15QKEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae03e51080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "trainX = tf.constant(cln_train_image_data, dtype = tf.float32)\n",
    "# only attempting 1 out of 30 facial locations (column 0), 'left_eye_center_x'\n",
    "trainY = tf.constant(cln_train_image_labels[:,0], dtype = tf.float32)\n",
    "learningRate = tf.constant(0.0001)\n",
    "# create beta variable (vector of 9216 beta values: 1 for each pixel)\n",
    "beta = tf.get_variable('beta', initializer=np.zeros((9216,1), dtype = np.float32))\n",
    "\n",
    "def model(this_beta, x):\n",
    "    return tf.matmul(x, this_beta)\n",
    "\n",
    "def cost(this_beta):\n",
    "    ans = tf.reduce_mean((model(this_beta, trainX) - trainY)**2) / 2\n",
    "    return ans\n",
    "\n",
    "cc = cost(beta)\n",
    "\n",
    "## Gradient descent by tensorflow\n",
    "gd = tf.train.GradientDescentOptimizer(learningRate)\n",
    "step = gd.minimize(cost(beta))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    cost_vec = []\n",
    "    # By running step multiple times, we perform gradient descent for the variable beta. \n",
    "    # For each step, we can ouput the cost function.\n",
    "    for i in range(500):\n",
    "        _, cost = sess.run([step, cc])\n",
    "        cost_vec.append(cost)\n",
    "        my_beta = sess.run([beta])[0]\n",
    "    plt.plot(cost_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Betas with the Development Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''REPEATING SAME TREATMENT OF DATA THAT WAS DONE TO THE TRAINING PORTION'''\n",
    "# initialize the development list of lists of pixel values \n",
    "dev_image_data = []\n",
    "\n",
    "# convert each list of pixel values (1 per row) to integers (first split the string up then convert each elem to int)\n",
    "for row in range(len(dev_data)):\n",
    "    dev_image_data.append([int(i) for i in list(dev_data[row][30].split())])\n",
    "# convert to a numpy array\n",
    "dev_image_data = np.array(dev_image_data)\n",
    "\n",
    "# convert range of 0 to 255 for pixel values to 0 to 1 (0 = white, 1 = black)\n",
    "dev_image_data = dev_image_data/255.0\n",
    "\n",
    "# initialize array of the 30 different facial location target values (labels) for each example image\n",
    "dev_image_labels = np.empty((len(dev_data), 30))\n",
    "\n",
    "# iterate through each row and each example image's 30 target facial locations and covert to floats or NaNs\n",
    "for i in range(len(dev_data)):\n",
    "    for j in range(len(dev_data[i][0:30])):\n",
    "        if dev_data[i][j] == '':\n",
    "            dev_image_labels[i][j] = float('nan')\n",
    "        else: \n",
    "            dev_image_labels[i][j] = (float(dev_data[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with NaN Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where 'left_eye_center_x' == NaN values from the dev and dev_label data\n",
    "non_NaN_list = np.invert((np.isnan(dev_image_labels[:,0])))\n",
    "cln_dev_image_data = dev_image_data[non_NaN_list]\n",
    "cln_dev_image_labels = dev_image_labels[non_NaN_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RMSE from Development Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.8318886191\n"
     ]
    }
   ],
   "source": [
    "# determine predicted x-coordiate for 'left_eye_center_x', named y_hat to match RMSE formula in Kaggle competition\n",
    "y_hat = np.matmul(cln_dev_image_data, my_beta)\n",
    "\n",
    "# y represents the original coordinate for left_eye_center_x'\n",
    "y = cln_dev_image_labels[:,0]\n",
    "\n",
    "n = len(y)\n",
    "\n",
    "# root mean squared error\n",
    "rmse = np.sqrt(np.sum((y[0] - y_hat)**2)/float(n))\n",
    "\n",
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
